<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="Untied Ulysses: Memory-Efficient Context Parallelism via Head-Level Chunking">
  <meta name="description" content="Untied Ulysses reduces peak attention activation memory by up to 82.5% for 70B Transformers and supports context lengths up to 5M tokens on a single 8xH100 node.">
  <meta name="keywords" content="context parallelism, long sequences, Transformers, activation memory, distributed training, attention, Ring Attention, Ulysses, memory efficiency, large language models">
  <meta name="author" content="Ravi Ghadia, Maksim Abraham, Sergei Vorobyov, Max Ryabinin">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Untied Ulysses">
  <meta property="og:title" content="Untied Ulysses: Memory-Efficient Context Parallelism via Head-Level Chunking">
  <meta property="og:description" content="Untied Ulysses reduces peak attention activation memory by up to 82.5% for 70B Transformers and supports context lengths up to 5M tokens on a single 8xH100 node.">
  <meta property="og:url" content="https://rghadia.github.io/untied_ulysses_proj">
  <meta property="og:image" content="https://rghadia.github.io/untied_ulysses_proj/static/images/main_figure.png">
  <meta property="og:image:width" content="2400">
  <meta property="og:image:height" content="1374">
  <meta property="og:image:alt" content="Untied Ulysses - Main Figure">
  <meta property="article:published_time" content="2025-01-01T00:00:00.000Z">
  <meta property="article:author" content="Ravi Ghadia">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="context parallelism">
  <meta property="article:tag" content="memory efficiency">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Untied Ulysses: Memory-Efficient Context Parallelism via Head-Level Chunking">
  <meta name="twitter:description" content="Untied Ulysses reduces peak attention activation memory by up to 82.5% for 70B Transformers and supports context lengths up to 5M tokens on a single 8xH100 node.">
  <meta name="twitter:image" content="https://rghadia.github.io/untied_ulysses_proj/static/images/main_figure.png">
  <meta name="twitter:image:alt" content="Untied Ulysses - Main Figure">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Untied Ulysses: Memory-Efficient Context Parallelism via Head-Level Chunking">
  <meta name="citation_author" content="Ghadia, Ravi">
  <meta name="citation_author" content="Abraham, Maksim">
  <meta name="citation_author" content="Vorobyov, Sergei">
  <meta name="citation_author" content="Ryabinin, Max">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="ICML 2025">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">

  <title>Untied Ulysses: Memory-Efficient Context Parallelism via Head-Level Chunking</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Untied Ulysses: Memory-Efficient Context Parallelism via Head-Level Chunking",
    "description": "Untied Ulysses reduces peak attention activation memory by up to 82.5% for 70B Transformers and supports context lengths up to 5M tokens on a single 8xH100 node.",
    "author": [
      {
        "@type": "Person",
        "name": "Ravi Ghadia"
      },
      {
        "@type": "Person",
        "name": "Maksim Abraham"
      },
      {
        "@type": "Person",
        "name": "Sergei Vorobyov"
      },
      {
        "@type": "Person",
        "name": "Max Ryabinin"
      }
    ],
    "datePublished": "2025",
    "publisher": {
      "@type": "Organization",
      "name": "ICML 2025"
    },
    "url": "https://rghadia.github.io/untied_ulysses_proj",
    "image": "https://rghadia.github.io/untied_ulysses_proj/static/images/main_figure.png",
    "keywords": ["context parallelism", "long sequences", "Transformers", "activation memory", "distributed training"],
    "abstract": "Efficiently processing long sequences with Transformer models usually requires splitting the computations across accelerators via context parallelism. The dominant approaches in this family of methods, such as Ring Attention or DeepSpeed Ulysses, enable scaling over the context dimension but do not focus on memory efficiency, which limits the sequence lengths they can support. More advanced techniques, such as Fully Pipelined Distributed Transformer or activation offloading, can further extend the possible context length at the cost of training throughput. In this paper, we present Untied Ulysses, a simple yet effective context parallelism technique that performs fine-grained chunking at the attention head level. This technique significantly reduces the activation memory usage of self-attention, breaking the activation memory barrier and unlocking much longer context lengths. Our approach lowers the peak activation memory usage in the attention layer by as much as 82.5% for 70B Transformers, while matching previous context parallelism techniques in terms of training speed. Untied Ulysses can support maximum context lengths of up to 5M tokens for training 8B models on a single 8xH100 node, improving upon prior methods by 25%.",
    "isAccessibleForFree": true
  }
  </script>
</head>
<body>

  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Untied Ulysses: Memory-Efficient Context Parallelism via Head-Level Chunking</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="#" target="_blank">Ravi Ghadia</a>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Maksim Abraham</a>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Sergei Vorobyov</a>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Max Ryabinin</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">ICML 2025</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Paper PDF (arXiv) -->
                <span class="link-block">
                  <a href="#" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (coming soon)</span>
                </a>
              </span>

              <!-- GitHub Code -->
              <span class="link-block">
                <a href="#" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code (coming soon)</span>
              </a>
            </span>

              <!-- arXiv abstract -->
              <span class="link-block">
                <a href="#" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv (coming soon)</span>
              </a>
            </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>
</section>


<!-- Main Figure -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/main_figure.png" alt="Untied Ulysses main figure: (a) DeepSpeed Ulysses communication and attention pattern showing head-parallel All-to-All operations, and (b) Untied Ulysses with memory-efficient head-level chunking that reuses QKV and All-to-All buffers to dramatically reduce activation memory." style="width: 100%; border-radius: 12px; box-shadow: 0 10px 15px -3px rgb(0 0 0 / 0.1);">
      <h2 class="subtitle has-text-centered" style="margin-top: 1.5rem;">
        <b>Figure:</b> (a) DeepSpeed Ulysses performs All-to-All communication followed by full multi-head attention. (b) <b>Untied Ulysses</b> processes attention heads in fine-grained chunks, reusing QKV and communication buffers to drastically reduce peak activation memory.
      </h2>
    </div>
  </div>
</section>
<!-- End main figure -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Efficiently processing long sequences with Transformer models usually requires splitting the computations across accelerators via context parallelism.
            The dominant approaches in this family of methods, such as Ring Attention or DeepSpeed Ulysses, enable scaling over the context dimension but do not focus on memory efficiency, which limits the sequence lengths they can support.
            More advanced techniques, such as Fully Pipelined Distributed Transformer or activation offloading, can further extend the possible context length at the cost of training throughput.
            In this paper, we present <b>Untied Ulysses</b>, a simple yet effective context parallelism technique that performs fine-grained chunking at the attention head level.
            This technique significantly reduces the activation memory usage of self-attention, breaking the activation memory barrier and unlocking much longer context lengths.
            Our approach lowers the peak activation memory usage in the attention layer by as much as <b>82.5%</b> for 70B Transformers, while matching previous context parallelism techniques in terms of training speed.
            Untied Ulysses can support maximum context lengths of up to <b>5M</b> tokens for training 8B models on a single 8xH100 node, improving upon prior methods by <b>25%</b>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/carousel1.jpg" alt="First research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel2.jpg" alt="Second research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel3.jpg" alt="Third research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">
      <img src="static/images/carousel4.jpg" alt="Fourth research result visualization" loading="lazy"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->



<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- Replace with your YouTube video ID when available -->
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" controls muted loop height="100%" preload="metadata">
            <source src="static/videos/carousel1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" controls muted loop height="100%" preload="metadata">
            <source src="static/videos/carousel2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" controls muted loop height="100%" preload="metadata">
            <source src="static/videos/carousel3.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->


<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>
      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
      </div>
    </div>
  </section>
<!--End paper poster -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@inproceedings{ghadia2025untied,
  title={Untied Ulysses: Memory-Efficient Context Parallelism via Head-Level Chunking},
  author={Ravi Ghadia and Maksim Abraham and Sergei Vorobyov and Max Ryabinin},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  </main>

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

  </body>
  </html>
