<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking">
  <meta name="description" content="Untied Ulysses reduces peak attention activation memory by up to 82.5% for 32B Transformers and supports context lengths up to 5M tokens on a single 8&times;H100 node.">
  <meta name="keywords" content="context parallelism, long sequences, Transformers, activation memory, distributed training, attention, Ring Attention, Ulysses, memory efficiency, large language models">
  <meta name="author" content="Ravi Ghadia, Maksim Abraham, Sergei Vorobyov, Max Ryabinin">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Untied Ulysses">
  <meta property="og:title" content="Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking">
  <meta property="og:description" content="Untied Ulysses reduces peak attention activation memory by up to 82.5% for 32B Transformers and supports context lengths up to 5M tokens on a single 8&times;H100 node.">
  <meta property="og:url" content="https://rghadia.github.io/untied_ulysses_proj">
  <meta property="og:image" content="https://rghadia.github.io/untied_ulysses_proj/static/images/main_figure.png">
  <meta property="og:image:width" content="2400">
  <meta property="og:image:height" content="1374">
  <meta property="og:image:alt" content="Untied Ulysses - Main Figure">
  <meta property="article:published_time" content="2025-01-01T00:00:00.000Z">
  <meta property="article:author" content="Ravi Ghadia">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="context parallelism">
  <meta property="article:tag" content="memory efficiency">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking">
  <meta name="twitter:description" content="Untied Ulysses reduces peak attention activation memory by up to 82.5% for 32B Transformers and supports context lengths up to 5M tokens on a single 8&times;H100 node.">
  <meta name="twitter:image" content="https://rghadia.github.io/untied_ulysses_proj/static/images/main_figure.png">
  <meta name="twitter:image:alt" content="Untied Ulysses - Main Figure">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking">
  <meta name="citation_author" content="Ghadia, Ravi">
  <meta name="citation_author" content="Abraham, Maksim">
  <meta name="citation_author" content="Vorobyov, Sergei">
  <meta name="citation_author" content="Ryabinin, Max">
  <meta name="citation_publication_date" content="2026">
  <meta name="citation_conference_title" content="ArXiv 2026">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">

  <title>Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Outfit:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking",
    "description": "Untied Ulysses reduces peak attention activation memory by up to 82.5% for 32B Transformers and supports context lengths up to 5M tokens on a single 8&times;H100 node.",
    "author": [
      {
        "@type": "Person",
        "name": "Ravi Ghadia"
      },
      {
        "@type": "Person",
        "name": "Maksim Abraham"
      },
      {
        "@type": "Person",
        "name": "Sergei Vorobyov"
      },
      {
        "@type": "Person",
        "name": "Max Ryabinin"
      }
    ],
    "datePublished": "2026",
    "publisher": {
      "@type": "Organization",
      "name": "ArXiv 2026"
    },
    "url": "https://rghadia.github.io/untied_ulysses_proj",
    "image": "https://rghadia.github.io/untied_ulysses_proj/static/images/main_figure.png",
    "keywords": ["context parallelism", "long sequences", "Transformers", "activation memory", "distributed training"],
    "abstract": "Efficiently processing long sequences with Transformer models usually requires splitting the computations across accelerators via context parallelism. The dominant approaches in this family of methods, such as Ring Attention or DeepSpeed Ulysses, enable scaling over the context dimension but do not focus on memory efficiency, which limits the sequence lengths they can support. More advanced techniques, such as Fully Pipelined Distributed Transformer or activation offloading, can further extend the possible context length at the cost of training throughput. In this paper, we present Upipe, a simple yet effective context parallelism technique that performs fine-grained chunking at the attention head level. This technique significantly reduces the activation memory usage of self-attention, breaking the activation memory barrier and unlocking much longer context lengths. Our approach lowers the peak activation memory usage in the attention layer by as much as 82.5% for 32B Transformers, while matching previous context parallelism techniques in terms of training speed. Untied Ulysses can support maximum context lengths of up to 5M tokens for training 8B models on a single 8&times;H100 node, improving upon prior methods by 25%.",
    "isAccessibleForFree": true
  }
  </script>
</head>
<body>

  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://ghadiaravi13.github.io/" target="_blank">Ravi Ghadia</a>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/fdrose" target="_blank">Maksim Abraham</a>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/vorobyov01/" target="_blank">Sergei Vorobyov</a>,</span>
              <span class="author-block">
                <a href="https://mryab.github.io/" target="_blank">Max Ryabinin</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">ArXiv 2026</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Paper PDF (arXiv) -->
                <span class="link-block">
                  <a href="#" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (coming soon)</span>
                </a>
              </span>

              <!-- GitHub Code -->
              <span class="link-block">
                <a href="#" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code (coming soon)</span>
              </a>
            </span>

              <!-- arXiv abstract -->
              <span class="link-block">
                <a href="#" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv (coming soon)</span>
              </a>
            </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Efficiently processing long sequences with Transformer models usually requires splitting the computations across accelerators via context parallelism.
            The dominant approaches in this family of methods, such as Ring Attention or DeepSpeed Ulysses, enable scaling over the context dimension but do not focus on memory efficiency, which limits the sequence lengths they can support.
            More advanced techniques, such as Fully Pipelined Distributed Transformer or activation offloading, can further extend the possible context length at the cost of training throughput.
            In this paper, we present <b>Upipe</b>, a simple yet effective context parallelism technique that performs fine-grained chunking at the attention head level.
            This technique significantly reduces the activation memory usage of self-attention, breaking the activation memory barrier and unlocking much longer context lengths.
            Our approach lowers the peak activation memory usage in the attention layer by as much as <b>82.5%</b> for 32B Transformers, while matching previous context parallelism techniques in terms of training speed.
            UPipe can support maximum context lengths of up to <b>5M</b> tokens for training 8B models on a 8&times;H100 node, improving upon prior methods by <b>25%</b>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Motivation Figure -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">The Memory Wall</h2>
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <p class="has-text-justified">The demand for training on longer sequences of data is growing rapidly, fueled by modern applications such as code-assistance and video-generation. However, training on very long sequences has a prohibitive memory cost, motivating the need for memory-efficient context parallelism. The figure below shows how the memory usage changes with different optimization techniques applied to a Llama3-8B model.</p>
          <br>
          <p class="has-text-justified">While techniques like FSDP helps lower the memory demand due to model parameters and the associated gradients, it does not address the activation memory footprint. DeepSpeed Ulysses is a context parallelism scheme that distributes the input sequence across 
            multiple devices, helping lower the activation memory footprint. ALST addresses the activation memory for non-attention stages via tiling, but overlooks the attention stage. Thus, in our work, we present <b>UPipe</b>, which addresses the attention stage by performing fine-grained chunking at the attention head level.
          </p>
          <br>
          <img src="static/images/intro_memory_stack.png" alt="Activation memory breakdown showing that attention dominates memory usage in long-context Transformer training." style="width: 100%; border-radius: 12px; box-shadow: 0 4px 6px -1px rgb(0 0 0 / 0.1);" loading="lazy"/>
          <h2 class="subtitle has-text-justified" style="margin-top: 1rem;">
            <!-- <b>Figure:</b>  -->
            <b> Llama3-8B:</b> Activation memory breakdown for context length of 3M training on 8&times;H100. Attention activations dominate the overall memory footprint, motivating the need for memory-efficient context parallelism.
            <b>ALST</b> handles activation memory for non-attention stages via tiling, but overlooks the attention stage. <b>UPipe</b> lowers the peak activation memory usage in the attention stage by headwise chunking.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End motivation figure -->

<!-- Method -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Untying the Attention Heads</h2>
        <div class="content has-text-justified">
          <p>
            <b>DeepSpeed Ulysses</b>, a widely used context parallelism technique, distributes the sequence across devices and uses All-to-All communication to exchange key-value pairs before computing full multi-head attention. While effective for scaling, this method materializes all attention head activations simultaneously, leading to high peak memory that limits the maximum context length.
          </p>
          <p>
            <b>Untied Ulysses (UPipe)</b> addresses this by performing fine-grained chunking at the attention head level. Instead of computing all heads at once, we process a subset of attention heads at a time, reusing QKV projection and All-to-All communication buffers across chunks. This simple modification dramatically reduces the peak activation memory in the attention layer without affecting the mathematical correctness of the computation.
          </p>
          <p>
            In the example depicted below for H = 4 heads, DeepSpeed Ulysses processes all 4 heads at once, thus requiring memory buffers for all 4 heads of QKV as well as the corresponding all-to-all buffers. UPipe, on the other hand, processes the heads in chunks of U = 2. Thus, it requires H/U = 2 stages to perform attention, where it reuses the memory buffers from the previous stage to store intermediate tensors in the later stages.
          </p>
        </div>
      </div>
    </div>
    <div style="margin-top: 1rem;">
      <img src="static/images/main_figure.png" alt="Untied Ulysses method overview: headwise chunking with buffer reuse." style="width: 100%; border-radius: 12px; box-shadow: 0 4px 6px -1px rgb(0 0 0 / 0.1);">
      <h2 class="subtitle has-text-centered" style="margin-top: 1rem;">
        <b>Figure:</b> (a) DeepSpeed Ulysses performs QKV projection and All-to-All communication followed by full multi-head attention. (b) <b>UPipe</b> processes attention heads in fine-grained chunks, reusing QKV and communication buffers to reduce peak activation memory usage, thereby allowing for longer context lengths.
      </h2>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">GQA Scheduling</h2>
        <div class="content has-text-justified">
          <p>
            To maintain compatibility with Grouped Query Attention (GQA), we implement a GQA-aware scheduling strategy that allows for headwise chunking. As shown in the figure, we schedule attention for heads out of order, to avoid redundant communication of KV in subsequent stages. 
          </p>
          <p>
            For example, in stage-0, we perform attention for KV heads 0,1,2,3 and correspondingly, pick the first query heads from the respective groups Q0, Q4, Q8, Q12. In stage-1, we no longer need to communicate KV heads 0,1,2,3 since they were already communicated in stage-0, and are already present on the correct devices. So we now pick the next query heads Q1, Q5, Q9, Q13. Thus we saved the communication for the KV in stages 1-3, reducing runtime and memory overhead.
          </p>
        </div>
      </div>
    </div>
    <div style="margin-top: 1rem;">
      <img src="static/images/GQA_fig_cropped.png" alt="GQA Scheduling" style="width: 100%; border-radius: 12px; box-shadow: 0 4px 6px -1px rgb(0 0 0 / 0.1);">
      <h2 class="subtitle has-text-centered" style="margin-top: 1rem;">
        <b>GQA Scheduling:</b> UPipe schedules attention for different QKV heads out of order, to avoid redundant communication of KV in subsequent stages. Finally, we rearrange the output heads to maintain correctness.
      </h2>
    </div>
  </div>
</section>
<!-- End method -->


<!-- Results -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Results</h2>
      <div class="content has-text-justified">
        <p>
          We evaluate Untied Ulysses against several context parallelism baselines on two model families across a wide range of context lengths (128K&ndash;5M tokens) on 8&times;H100 nodes. Training throughput is reported in tokens per second per GPU. <b>Bold</b> values indicate the best result for each context length.
        </p>
      </div>

      <!-- Llama3-8B Table -->
      <div class="content">
        <h3 class="title is-4" style="margin-bottom: 0.75rem;">Llama3-8B (8&times;H100)</h3>
        <div style="overflow-x: auto;">
          <table class="table is-bordered is-striped is-hoverable is-fullwidth" style="font-size: 0.95rem;">
            <thead>
              <tr>
                <th>Method</th>
                <th>128K</th>
                <th>256K</th>
                <th>512K</th>
                <th>1M</th>
                <th>2M</th>
                <th>3M</th>
                <th>4M</th>
                <th>5M</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Native PyTorch</td>
                <td>1373.87</td>
                <td>845.99</td>
                <td>474.30</td>
                <td>249.85</td>
                <td>OOM</td>
                <td>&mdash;</td>
                <td>&mdash;</td>
                <td>&mdash;</td>
              </tr>
              <tr>
                <td>Ring</td>
                <td>2064.90</td>
                <td>1387.67</td>
                <td>841.05</td>
                <td>458.51</td>
                <td>237.99</td>
                <td>159.96</td>
                <td>OOM</td>
                <td>&mdash;</td>
              </tr>
              <tr>
                <td>Ulysses</td>
                <td><b>2320.47</b></td>
                <td><b>1503.80</b></td>
                <td><b>878.63</b></td>
                <td><b>475.33</b></td>
                <td>246.05</td>
                <td>162.41</td>
                <td>OOM</td>
                <td>&mdash;</td>
              </tr>
              <tr>
                <td>FPDT</td>
                <td>1171.68</td>
                <td>884.75</td>
                <td>621.20</td>
                <td>382.42</td>
                <td>219.53</td>
                <td>153.48</td>
                <td>119.76</td>
                <td>&mdash;</td>
              </tr>
              <tr style="background-color: #eef6ff;">
                <td><b>UPipe</b></td>
                <td>2281.05</td>
                <td>1487.29</td>
                <td>867.17</td>
                <td>472.53</td>
                <td><b>246.07</b></td>
                <td><b>166.32</b></td>
                <td><b>125.56</b></td>
                <td><b>98.25</b></td>
              </tr>
            </tbody>
          </table>
        </div>
      </div>

      <!-- Qwen3-32B Table -->
      <div class="content" style="margin-top: 2rem;">
        <h3 class="title is-4" style="margin-bottom: 0.75rem;">Qwen3-32B (16&times;H100)</h3>
        <div style="overflow-x: auto;">
          <table class="table is-bordered is-striped is-hoverable is-fullwidth" style="font-size: 0.95rem;">
            <thead>
              <tr>
                <th>Method</th>
                <th>128K</th>
                <th>256K</th>
                <th>512K</th>
                <th>1M</th>
                <th>2M</th>
                <th>3M</th>
                <th>4M</th>
                <th>5M</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Native PyTorch</td>
                <td>127.03</td>
                <td>112.20</td>
                <td>91.39</td>
                <td>OOM</td>
                <td>&mdash;</td>
                <td>&mdash;</td>
                <td>&mdash;</td>
                <td>&mdash;</td>
              </tr>
              <tr>
                <td>Ring</td>
                <td>418.39</td>
                <td>308.88</td>
                <td>194.44</td>
                <td>110.27</td>
                <td>58.45</td>
                <td>OOM</td>
                <td>&mdash;</td>
                <td>&mdash;</td>
              </tr>
              <tr>
                <td>Ulysses</td>
                <td><b>545.29</b></td>
                <td><b>370.70</b></td>
                <td><b>217.04</b></td>
                <td><b>117.02</b></td>
                <td><b>59.98</b></td>
                <td>OOM</td>
                <td>&mdash;</td>
                <td>&mdash;</td>
              </tr>
              <tr>
                <td>FPDT</td>
                <td>286.40</td>
                <td>217.85</td>
                <td>151.91</td>
                <td>95.88</td>
                <td>55.41</td>
                <td>38.86</td>
                <td>27.66</td>
                <td>&mdash;</td>
              </tr>
              <tr style="background-color: #eef6ff;">
                <td><b>UPipe</b></td>
                <td>483.29</td>
                <td>339.56</td>
                <td>204.46</td>
                <td>113.26</td>
                <td><b>59.56</b></td>
                <td><b>40.42</b></td>
                <td><b>29.97</b></td>
                <td>OOM</td>
              </tr>
            </tbody>
          </table>
        </div>
      </div>

      <div class="content has-text-justified" style="margin-top: 1.5rem;">
        <p>
          On <b>Llama3-8B</b>, Untied Ulysses matches the throughput of DeepSpeed Ulysses at shorter context lengths while being the <em>only</em> method that can scale to <b>5M tokens</b>&mdash;a 25% improvement over the next best method (FPDT at 4M). At 2M+ tokens, Untied Ulysses consistently achieves the highest throughput among all methods.
        </p>
        <p>
          On the larger <b>Qwen3-32B</b> model, UPipe achieves on par performance with DeepSpeed Ulysses, while consistently outperforms FPDT at all context lengths. UPipe also achieve a 2&times; longer context length over DeepSpeed Ulysses.
        </p>
      </div>

      <div class="columns" style="margin-top: 2rem;">
        <div class="column is-half">
          <div class="content has-text-justified">
            <h3 class="title is-4">Llama3-8B Multi-node (16&times;H100)</h3>
            <p>
              We evaluate the performance of UPipe using Llama3-8B on 16&times;H100 GPUs. 
              We use a hybrid context parallelism setup similar to USP-Hybrid, with <i>8-ulysses-2-ring</i>, ie, Ulysses degree of 8 within the node, and ring degree of 2 across nodes.
            <!-- </p>
            <p> -->
              The figure on the right compares the training throughput and memory usage across methods as context length scales. 
              UPipe consistently achieves throughput on par with DeepSpeed Ulysses, while using less memory. Further, UPipe is able to support a maximum context length of <b>8M tokens</b>, improving upon USP-Hybrid's 6M tokens by <b>33%</b>.
            </p>
          </div>
        </div>
        <div class="column is-half">
          <img src="static/images/multinode_mem.png" alt="Multi-node memory comparison across methods" style="width: 96%; padding-left: 4px; margin-top: 4.2rem;">
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End results -->


<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            Replace with your YouTube video ID when available 
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" controls muted loop height="100%" preload="metadata">
            <source src="static/videos/carousel1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" controls muted loop height="100%" preload="metadata">
            <source src="static/videos/carousel2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" controls muted loop height="100%" preload="metadata">
            <source src="static/videos/carousel3.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->


<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>
      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
      </div>
    </div>
  </section> -->
<!--End paper poster -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{ghadia2026untied,
  title={Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking},
  author={Ravi Ghadia and Maksim Abraham and Sergei Vorobyov and Max Ryabinin},
  booktitle={ArXiv 2026},
  year={2026}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  </main>

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

  </body>
  </html>
